{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apex==0.9.10dev (from -r requirements.txt (line 1))\n",
      "  Downloading apex-0.9.10dev.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bagua==0.9.2 (from -r requirements.txt (line 2))\n",
      "  Downloading bagua-0.9.2.tar.gz (916 kB)\n",
      "     ---------------------------------------- 0.0/916.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 916.6/916.6 kB 20.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting clearml==1.16.4 (from -r requirements.txt (line 3))\n",
      "  Downloading clearml-1.16.4-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting cx_Freeze==7.2.0 (from -r requirements.txt (line 4))\n",
      "  Downloading cx_Freeze-7.2.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting einops==0.8.0 (from -r requirements.txt (line 5))\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock==3.15.4 (from -r requirements.txt (line 6))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fire==0.6.0 (from -r requirements.txt (line 7))\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fvcore==0.1.5.post20221221 (from -r requirements.txt (line 8))\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ignite==1.1.0 (from -r requirements.txt (line 9))\n",
      "  Downloading ignite-1.1.0-py2.py3-none-any.whl.metadata (856 bytes)\n",
      "Collecting itk==5.4.0 (from -r requirements.txt (line 10))\n",
      "  Downloading itk-5.4.0-cp311-abi3-win_amd64.whl.metadata (22 kB)\n",
      "Collecting lz4==4.3.2 (from -r requirements.txt (line 11))\n",
      "  Downloading lz4-4.3.2.tar.gz (170 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 12))\n",
      "  Downloading matplotlib-3.8.4-cp312-cp312-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting mlflow==2.15.1 (from -r requirements.txt (line 13))\n",
      "  Downloading mlflow-2.15.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting nibabel==5.2.1 (from -r requirements.txt (line 14))\n",
      "  Downloading nibabel-5.2.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting nnunetv2==2.5.1 (from -r requirements.txt (line 15))\n",
      "  Downloading nnunetv2-2.5.1.tar.gz (196 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy==1.24.4 (from -r requirements.txt (line 16))\n",
      "  Downloading numpy-1.24.4.tar.gz (10.9 MB)\n",
      "     ---------------------------------------- 0.0/10.9 MB ? eta -:--:--\n",
      "     ---------------------------- ----------- 7.9/10.9 MB 40.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.9/10.9 MB 29.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "          backend = _build_backend()\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "          obj = import_module(mod_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "        File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\Temp\\3\\pip-build-env-s435bpos\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "          import setuptools.version\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\Temp\\3\\pip-build-env-s435bpos\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "          import pkg_resources\n",
      "        File \"C:\\Users\\scmmw\\AppData\\Local\\Temp\\3\\pip-build-env-s435bpos\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                          ^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardXNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboardX) (24.1)\n",
      "Collecting protobuf>=3.20 (from tensorboardX)\n",
      "  Downloading protobuf-5.27.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Downloading protobuf-5.27.4-cp310-abi3-win_amd64.whl (426 kB)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-5.27.4 tensorboardX-2.6.2.2\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai==1.3.2\n",
      "  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from monai==1.3.2) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from monai==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.9->monai==1.3.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.9->monai==1.3.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.9->monai==1.3.2) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.9->monai==1.3.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.9->monai==1.3.2) (3.1.4)\n",
      "Collecting fsspec (from torch>=1.9->monai==1.3.2)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.9->monai==1.3.2) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.9->monai==1.3.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\scmmw\\appdata\\local\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=1.9->monai==1.3.2) (1.3.0)\n",
      "Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 23.5 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: fsspec, monai\n",
      "Successfully installed fsspec-2024.6.1 monai-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge monai=1.3.2\n",
    "!pip install monai==1.3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.4.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: c:\\Users\\<username>\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.24.0\n",
      "scipy version: 1.14.1\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.17.1\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.19.0\n",
      "tqdm version: 4.66.5\n",
      "lmdb version: 1.5.1\n",
      "psutil version: 5.9.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.15.1\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.16.4\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for logging, file handling, and system operations\n",
    "import logging  # Provides a flexible framework for emitting log messages from Python programs.\n",
    "import os  # Provides a way of using operating system-dependent functionality, such as reading or writing to the file system.\n",
    "import shutil  # Used for high-level file operations, such as copying or removing files and directories.\n",
    "import sys  # Provides access to some variables used or maintained by the Python interpreter and to functions that interact with the interpreter.\n",
    "import tempfile  # Used to create temporary files and directories.\n",
    "import random  # Implements pseudo-random number generators for various distributions.\n",
    "import numpy as np  # Fundamental package for scientific computing with Python, providing support for large, multi-dimensional arrays and matrices.\n",
    "from tqdm import trange  # Provides a progress bar for loops, making it easier to track long-running tasks.\n",
    "import matplotlib.pyplot as plt  # A plotting library used for creating static, animated, and interactive visualizations in Python.\n",
    "import torch  # PyTorch, an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing.\n",
    "#from skimage.util import random_noise  # A function to add random noise of various types to an image.\n",
    "# Importing functions and classes from the MONAI library, a deep learning framework specialized for healthcare imaging\n",
    "from monai.apps import download_and_extract  # Utility to download and extract files, particularly useful for datasets.\n",
    "from monai.config import print_config  # Prints the current configuration of MONAI, including the environment, installed packages, and versions.\n",
    "from monai.data import CacheDataset, DataLoader  # CacheDataset caches data and is useful for datasets that fit in memory. DataLoader is used to load the data in batches.\n",
    "from monai.networks.nets import AutoEncoder, SwinUNETR  # Importing an AutoEncoder model, which is a type of neural network used for unsupervised learning, particularly for dimensionality reduction.\n",
    "from monai.transforms import (  # Importing various transformations to be applied to the data.\n",
    "    EnsureChannelFirstD,  # Ensures the channel dimension is first in the data tensor.\n",
    "    Compose,  # Allows the chaining of multiple transformations to be applied sequentially.\n",
    "    LoadImageD,  # Loads images from a file.\n",
    "    RandFlipD,  # Randomly flips the image along a specified axis.\n",
    "    RandRotateD,  # Randomly rotates the image within a specified angle range.\n",
    "    RandZoomD,  # Randomly zooms in or out of the image within a specified range.\n",
    "    ScaleIntensityD,  # Scales the intensity of the image to a specified range.\n",
    "    EnsureTypeD,  # Ensures the output is of a specific data type.\n",
    "    Lambda,  # Allows for custom transformations using a lambda function.\n",
    "    #AddChannelD\n",
    ")\n",
    "from monai.utils import set_determinism  # Sets the seed for random number generators to ensure reproducibility.\n",
    "from monai.networks.utils import copy_model_state\n",
    "from monai.networks.nets.swin_unetr import filter_swinunetr\n",
    "from monai.transforms import RandSpatialCropd\n",
    "import math\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR, _LRScheduler\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "# Print the current MONAI configuration\n",
    "print_config()\n",
    "#from Hand_Scan_Dataset import HandScanDataset2, train_df, valid_df, training_data_dir\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import psutil  # For memory tracking\n",
    "import gc\n",
    "from torch.cuda.amp import autocast\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "# Standard library imports\n",
    "from pathlib import Path  # To handle and manipulate filesystem paths\n",
    "import os  # For interacting with the operating system\n",
    "import glob  # For finding all file paths matching a specified pattern\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np  # For numerical operations and handling arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n",
    "from PIL import Image  # For opening, manipulating, and saving many different image file formats\n",
    "\n",
    "# PyTorch imports\n",
    "import torch  # Main PyTorch library for building and training neural networks\n",
    "from torch.utils.data import Dataset, DataLoader  # For handling datasets and data loaders\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch-I/O extension\n",
    "import torchio as tio  # For medical image processing in PyTorch\n",
    "\n",
    "# pydicom imports\n",
    "import pydicom  # For reading, modifying, and writing DICOM files\n",
    "from pydicom.data import get_testdata_file  # For accessing test DICOM files\n",
    "from pydicom.fileset import FileSet  # For working with DICOM FileSets\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split  # For splitting datasets into training and testing sets\n",
    "\n",
    "from collections import defaultdict\n",
    "from monai.transforms import apply_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basic logging configuration to output log messages to the console (stdout).\n",
    "# The logging level is set to INFO, meaning all messages at this level and above\n",
    "# (INFO, WARNING, ERROR, CRITICAL) will be displayed.\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "\n",
    "# Set deterministic behavior for reproducibility. This is important in experiments where\n",
    "# you want to ensure that the results are the same every time the code is run. \n",
    "# The seed value is set to 0, which will be used to initialize the random number generator.\n",
    "set_determinism(0)\n",
    "\n",
    "# Determine the device to run the computations on. If a CUDA-capable GPU is available,\n",
    "# the device will be set to \"cuda\" (meaning GPU); otherwise, it will fall back to \"cpu\".\n",
    "# This allows the code to take advantage of GPU acceleration if possible.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_ims(ims, shape=None, figsize=(10, 10), titles=None):\n",
    "    shape = (1, len(ims)) if shape is None else shape\n",
    "    plt.subplots(*shape, figsize=figsize)\n",
    "    for i, im in enumerate(ims):\n",
    "        plt.subplot(*shape, i + 1)\n",
    "        \n",
    "        # Convert numpy array to a compatible data type if needed\n",
    "        if isinstance(im, np.ndarray):\n",
    "            im = im.astype(np.float32)  # Convert to a supported type like float32\n",
    "            im = torch.from_numpy(im)\n",
    "        \n",
    "        # Squeeze the image if it's a tensor\n",
    "        if isinstance(im, torch.Tensor):\n",
    "            im = torch.squeeze(im)\n",
    "        \n",
    "        plt.imshow(im, cmap=\"gray\")\n",
    "        \n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define a custom transformation using MONAI\\'s Lambda transform.\\n# The transformation applies two types of noise to the image: Gaussian noise and Salt & Pepper noise.\\n# It creates a dictionary with three keys: \"orig\" (original image), \"gaus\" (image with Gaussian noise),\\n# and \"s&p\" (image with Salt & Pepper noise).\\nNoiseLambda = Lambda(\\n    lambda d: {\\n        \"orig\": d[\"im\"],  # Original image\\n        \"gaus\": torch.tensor(random_noise(d[\"im\"], mode=\"gaussian\"), dtype=torch.float32),  # Gaussian noise\\n        \"s&p\": torch.tensor(random_noise(d[\"im\"], mode=\"s&p\", salt_vs_pepper=0.1)),  # Salt & Pepper noise\\n    }\\n)\\n\\n# Define the set of transformations to apply to the training data.\\ntrain_transforms = Compose(\\n    [\\n        LoadImageD(keys=[\"im\"]),  # Load the image from file\\n        EnsureChannelFirstD(keys=[\"im\"]),  # Ensure the image has the channel dimension first (e.g., [C, H, W])\\n        ScaleIntensityD(keys=[\"im\"]),  # Scale the intensity of the image to a specific range (default 0 to 1)\\n        RandRotateD(keys=[\"im\"], range_x=np.pi / 12, prob=0.5, keep_size=True),  # Randomly rotate the image\\n        RandFlipD(keys=[\"im\"], spatial_axis=0, prob=0.5),  # Randomly flip the image along the specified axis\\n        RandZoomD(keys=[\"im\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),  # Randomly zoom in/out on the image\\n        EnsureTypeD(keys=[\"im\"]),  # Ensure the image is a PyTorch tensor with appropriate data type\\n        NoiseLambda,  # Apply the custom noise transformation\\n    ]\\n)\\n\\n# Define the set of transformations to apply to the test data.\\ntest_transforms = Compose(\\n    [\\n        LoadImageD(keys=[\"im\"]),  # Load the image from file\\n        EnsureChannelFirstD(keys=[\"im\"]),  # Ensure the image has the channel dimension first\\n        ScaleIntensityD(keys=[\"im\"]),  # Scale the intensity of the image to a specific range\\n        EnsureTypeD(keys=[\"im\"]),  # Ensure the image is a PyTorch tensor with appropriate data type\\n        NoiseLambda,  # Apply the custom noise transformation\\n    ]\\n)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define a custom transformation using MONAI's Lambda transform.\n",
    "# The transformation applies two types of noise to the image: Gaussian noise and Salt & Pepper noise.\n",
    "# It creates a dictionary with three keys: \"orig\" (original image), \"gaus\" (image with Gaussian noise),\n",
    "# and \"s&p\" (image with Salt & Pepper noise).\n",
    "NoiseLambda = Lambda(\n",
    "    lambda d: {\n",
    "        \"orig\": d[\"im\"],  # Original image\n",
    "        \"gaus\": torch.tensor(random_noise(d[\"im\"], mode=\"gaussian\"), dtype=torch.float32),  # Gaussian noise\n",
    "        \"s&p\": torch.tensor(random_noise(d[\"im\"], mode=\"s&p\", salt_vs_pepper=0.1)),  # Salt & Pepper noise\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the set of transformations to apply to the training data.\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImageD(keys=[\"im\"]),  # Load the image from file\n",
    "        EnsureChannelFirstD(keys=[\"im\"]),  # Ensure the image has the channel dimension first (e.g., [C, H, W])\n",
    "        ScaleIntensityD(keys=[\"im\"]),  # Scale the intensity of the image to a specific range (default 0 to 1)\n",
    "        RandRotateD(keys=[\"im\"], range_x=np.pi / 12, prob=0.5, keep_size=True),  # Randomly rotate the image\n",
    "        RandFlipD(keys=[\"im\"], spatial_axis=0, prob=0.5),  # Randomly flip the image along the specified axis\n",
    "        RandZoomD(keys=[\"im\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),  # Randomly zoom in/out on the image\n",
    "        EnsureTypeD(keys=[\"im\"]),  # Ensure the image is a PyTorch tensor with appropriate data type\n",
    "        NoiseLambda,  # Apply the custom noise transformation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the set of transformations to apply to the test data.\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImageD(keys=[\"im\"]),  # Load the image from file\n",
    "        EnsureChannelFirstD(keys=[\"im\"]),  # Ensure the image has the channel dimension first\n",
    "        ScaleIntensityD(keys=[\"im\"]),  # Scale the intensity of the image to a specific range\n",
    "        EnsureTypeD(keys=[\"im\"]),  # Ensure the image is a PyTorch tensor with appropriate data type\n",
    "        NoiseLambda,  # Apply the custom noise transformation\n",
    "    ]\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "NoiseLambda = Lambda(\n",
    "    lambda d: {\n",
    "        \"im\": d[\"im\"],  # Preserve the original image key\n",
    "        \"orig\": d[\"im\"].clone(),  # Store a copy of the original image\n",
    "        \"gaus\": torch.tensor(random_noise(d[\"im\"].numpy(), mode=\"gaussian\"), dtype=torch.float32),\n",
    "        \"s&p\": torch.tensor(random_noise(d[\"im\"].numpy(), mode=\"s&p\", salt_vs_pepper=0.1), dtype=torch.float32),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        AddChannelD(keys=[\"im\"]),\n",
    "        ScaleIntensityD(keys=[\"im\"]),\n",
    "        RandRotateD(keys=[\"im\"], range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlipD(keys=[\"im\"], spatial_axis=0, prob=0.5),\n",
    "        RandZoomD(keys=[\"im\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        EnsureTypeD(keys=[\"im\"]),\n",
    "        NoiseLambda,\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        AddChannelD(keys=[\"im\"]),\n",
    "        ScaleIntensityD(keys=[\"im\"]),\n",
    "        EnsureTypeD(keys=[\"im\"]),\n",
    "        NoiseLambda,\n",
    "    ]\n",
    ")'''\n",
    "\n",
    "'''from monai.transforms import Compose, ScaleIntensityD, RandRotateD, RandFlipD, RandZoomD, EnsureTypeD, Lambda\n",
    "import torch\n",
    "import numpy as np\n",
    "from skimage.util import random_noise\n",
    "\n",
    "# Save the original image before applying transformations\n",
    "SaveOriginalLambda = Lambda(\n",
    "    lambda d: {\n",
    "        \"im\": d[\"im\"],  # Original image key\n",
    "        \"orig\": d[\"im\"].clone(),  # Save a clone of the original image\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add noise transformations while preserving the original\n",
    "NoiseLambda = Lambda(\n",
    "    lambda d: {\n",
    "        **d,  # Keep the original keys\n",
    "        \"gaus\": torch.tensor(random_noise(d[\"orig\"].numpy(), mode=\"gaussian\"), dtype=torch.float32),  # Gaussian noise on original\n",
    "        \"s&p\": torch.tensor(random_noise(d[\"orig\"].numpy(), mode=\"s&p\", salt_vs_pepper=0.1), dtype=torch.float32),  # Salt & Pepper noise on original\n",
    "    }\n",
    ")\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        # AddChannelD(keys=[\"im\"]),\n",
    "        SaveOriginalLambda,  # Save the original image before any transformations\n",
    "        ScaleIntensityD(keys=[\"im\"]),\n",
    "        RandRotateD(keys=[\"im\"], range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlipD(keys=[\"im\"], spatial_axis=0, prob=0.5),\n",
    "        RandZoomD(keys=[\"im\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        EnsureTypeD(keys=[\"im\"]),\n",
    "        #NoiseLambda,  # Apply noise to the stored original image\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        # AddChannelD(keys=[\"im\"]),\n",
    "        SaveOriginalLambda,  # Save the original image before applying noise\n",
    "        ScaleIntensityD(keys=[\"im\"]),\n",
    "        EnsureTypeD(keys=[\"im\"]),\n",
    "        #NoiseLambda,  # Apply noise to the stored original image\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "\n",
    "train_transforms = RandSpatialCropd(keys=[\"im\"], roi_size=(96, 96, 96), random_center=True, random_size=False)\n",
    "test_transforms = RandSpatialCropd(keys=[\"im\"], roi_size=(96, 96, 96), random_center=True, random_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the base directory and CSV path\n",
    "training_data_dir = r'c:\\Users\\scmmw\\OneDrive - University of Leeds\\t1_vibe_we_hand_subset'\n",
    "csv_path = r'C:\\Users\\scmmw\\OneDrive - University of Leeds\\t1_vibe_we_hand_subset\\training_labels_subset.csv'\n",
    "\n",
    "# Read the labels DataFrame\n",
    "labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, valid_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['progression'])\n",
    "\n",
    "# Save the splits for reference (optional)\n",
    "train_df.to_csv(os.path.join(training_data_dir, 'train_split.csv'), index=False)\n",
    "valid_df.to_csv(os.path.join(training_data_dir, 'valid_split.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct full paths to patient directories for training and validation\n",
    "train_patient_dirs = [os.path.join(training_data_dir, subject_name) for subject_name in train_df['patient ID'].tolist()]\n",
    "valid_patient_dirs = [os.path.join(training_data_dir, subject_name) for subject_name in valid_df['patient ID'].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HandScanDataset2(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patient_dirs (list): List of paths to the patient directories.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            device (torch.device, optional): Device to use for tensor operations (e.g., 'cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.data_dir = sorted(data_dir)\n",
    "        self.transform = transform\n",
    "        self.device = device if device else torch.device('cpu')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get the patient directory based on the index\n",
    "        patient_dir = self.data_dir[idx]\n",
    "\n",
    "        # Load and process images from the patient directory\n",
    "        images = self.get_best_patient_images(patient_dir)\n",
    "\n",
    "        if len(images) == 0:\n",
    "            raise ValueError(f\"No images found for patient directory {patient_dir}\")\n",
    "\n",
    "        # Correct use of autocast and tensor creation\n",
    "        with torch.amp.autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "            # Proper handling of tensor conversion\n",
    "            images = [\n",
    "                torch.tensor(img, dtype=torch.float32).to(self.device) if not isinstance(img, torch.Tensor) else img.to(self.device).float()\n",
    "                for img in images\n",
    "            ]\n",
    "            \n",
    "            images_tensor = torch.stack(images, dim=0).to(self.device)\n",
    "            images_tensor_channel = torch.unsqueeze(images_tensor, 0)  # Add a channel dimension\n",
    "\n",
    "\n",
    "        # Prepare the data dictionary with the image tensor\n",
    "        data = {\"im\": images_tensor_channel}\n",
    "\n",
    "        # Apply any provided transforms\n",
    "        if self.transform:\n",
    "            data = self.transform(data)  # Assuming transform takes and returns a dictionary\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_best_patient_images(self, base_path):\n",
    "        \"\"\" \n",
    "        Process all images in the 't1_vibe_we' subfolder of each subject.\n",
    "        Sort images by Instance Number and return a sequence of a fixed length.\n",
    "        \"\"\"\n",
    "        seq_len = 32\n",
    "        all_images = []\n",
    "        dicom_files = [] \n",
    "        target_size = (512, 512)  # Set a fixed image shape\n",
    "\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            if 't1_vibe_we' in dirs:\n",
    "                t1_vibe_we_path = os.path.join(root, 't1_vibe_we')\n",
    "                \n",
    "                # Get the images in the 't1_vibe_we' sequence\n",
    "                dicom_files = []\n",
    "                for image_path in glob.glob(os.path.join(t1_vibe_we_path, '*')):\n",
    "                    try:\n",
    "                        dicom_file = pydicom.dcmread(image_path)\n",
    "                        dicom_files.append((dicom_file, image_path))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "\n",
    "            # Sort the files by Instance Number\n",
    "            dicom_files.sort(key=lambda x: x[0].InstanceNumber)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            dicom_files = self.remove_duplicates(dicom_files)\n",
    "\n",
    "            # Find the best slice\n",
    "            if dicom_files:\n",
    "                # Find the slice with the highest intensity\n",
    "                max_sum = -1\n",
    "                best_dicom_file, best_image_path = None, None\n",
    "                for dicom_file, image_path in dicom_files:\n",
    "                    image = dicom_file.pixel_array\n",
    "                    image_sum = np.sum(image)\n",
    "                    if image_sum > max_sum:\n",
    "                        max_sum = image_sum\n",
    "                        best_dicom_file, best_image_path = dicom_file, image_path\n",
    "\n",
    "                if best_dicom_file is not None:\n",
    "                    best_instance_number = best_dicom_file.InstanceNumber\n",
    "\n",
    "                    # Calculate the central slice index\n",
    "                    central_index = best_instance_number - 1  # InstanceNumber is 1-based\n",
    "\n",
    "                    # Determine the range of slices to extract the central 5 slices\n",
    "                    start_index = max(0, central_index - 2)\n",
    "                    end_index = min(len(dicom_files), central_index + 3)\n",
    "\n",
    "                    # Extract the central 5 slices\n",
    "                    selected_slices = dicom_files[start_index:end_index]\n",
    "\n",
    "                    images = []\n",
    "                    for dicom_file, image_path in selected_slices:\n",
    "                        try:\n",
    "                            image = self.process_dicom_image(image_path, target_size=target_size)\n",
    "                            images.append(image)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "                    # Pad to the required sequence length if needed\n",
    "                    if len(images) < seq_len:\n",
    "                        # Pad with zero images of the same shape as the original images\n",
    "                        diff = seq_len - len(images)\n",
    "                        images.extend([torch.zeros(target_size, dtype=torch.float32).to(self.device) for _ in range(diff)])\n",
    "\n",
    "                    all_images.extend(images)\n",
    "\n",
    "        return all_images\n",
    "\n",
    "\n",
    "    def remove_duplicates(self, dicom_files):\n",
    "        \"\"\" Remove duplicate instance numbers, keeping only the slice with the highest sum of intensities. \"\"\"\n",
    "        instance_dict = defaultdict(list)\n",
    "\n",
    "        for dicom_file, image_path in dicom_files:\n",
    "            instance_number = dicom_file.InstanceNumber\n",
    "            instance_dict[instance_number].append((dicom_file, image_path))\n",
    "\n",
    "        # Compare DICOM files with the same Instance Number\n",
    "        unique_dicom_files = []\n",
    "        for instance_number, files in instance_dict.items():\n",
    "            if len(files) > 1:\n",
    "                best_slice = self.find_best_slice(files)\n",
    "                unique_dicom_files.append(best_slice)\n",
    "            else:\n",
    "                unique_dicom_files.append(files[0])\n",
    "\n",
    "        return unique_dicom_files\n",
    "\n",
    "    def find_best_slice(self, dicom_files):\n",
    "        \"\"\" Find the slice with the 'DOTAREM' ContrastBolusAgent or, as a fallback, return the first available slice. \"\"\"\n",
    "        best_slice = None\n",
    "\n",
    "        # Check for the slice with 'DOTAREM'\n",
    "        for dicom_file, image_path in dicom_files:\n",
    "            if hasattr(dicom_file, 'ContrastBolusAgent') and dicom_file.ContrastBolusAgent == 'DOTAREM':\n",
    "                best_slice = (dicom_file, image_path)\n",
    "                break  # Stop searching once we find the 'DOTAREM' slice\n",
    "\n",
    "        # Fallback: If no slice with 'DOTAREM' is found, return the first slice\n",
    "        if best_slice is None:\n",
    "            best_slice = dicom_files[0]\n",
    "\n",
    "        return best_slice\n",
    "\n",
    "    def process_dicom_image(self, path: str, resize=True, target_size=(512, 512)) -> torch.Tensor:\n",
    "        dicom_file = pydicom.dcmread(path)\n",
    "        image = torch.tensor(dicom_file.pixel_array, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Skip invalid images\n",
    "        if 0 in image.shape:\n",
    "            print(f\"Skipping image due to invalid shape: {image.shape}\")\n",
    "            return torch.zeros(target_size, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Use autocast to optimize tensor operations\n",
    "        with torch.amp.autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "            # Normalize the image: Zero mean and unit variance\n",
    "            mean = image.mean()\n",
    "            std = image.std()\n",
    "            image = (image - mean) / (std + 1e-7)\n",
    "\n",
    "            # Apply 95% clipping\n",
    "            lower_bound = torch.quantile(image, 0.025)\n",
    "            upper_bound = torch.quantile(image, 0.975)\n",
    "            image = torch.clamp(image, lower_bound, upper_bound)\n",
    "\n",
    "            # Normalize again after clipping\n",
    "            mean = image.mean()\n",
    "            std = image.std()\n",
    "            image = (image - mean) / (std + 1e-7)\n",
    "\n",
    "            # Resize the image to the target size\n",
    "            if resize:\n",
    "                image = image.unsqueeze(0)  # Add channel dimension for resizing\n",
    "                image = torch.nn.functional.interpolate(image.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "        \n",
    "        return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.data = []\n",
    "        for i in range(len(original_dataset)):\n",
    "            sample = original_dataset[i]\n",
    "            self.data.append(sample[\"im\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"im\": self.data[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device to be used for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0\n",
    "\n",
    "# Select a few subjects (e.g., the first three subjects)\n",
    "train_subset_df = train_patient_dirs[:80]\n",
    "# Initialize the dataset with the selected subjects\n",
    "train_ds = HandScanDataset2(data_dir=train_subset_df, transform=train_transforms, device=torch.device(\"cuda\"))\n",
    "# Create a data loader for testing\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Select a few subjects (e.g., the first three subjects)\n",
    "valid_subset_df = valid_patient_dirs[:20]\n",
    "# Initialize the dataset with the selected subjects\n",
    "train_dataset = HandScanDataset2( data_dir=valid_subset_df, transform=test_transforms, device=torch.device(\"cuda\"))\n",
    "# Create a data loader for testing\n",
    "test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Batch size and number of workers\\nbatch_size = 1\\nnum_workers = 0\\n\\n# Select a few subjects for training (e.g., the first 80 subjects)\\ntrain_subset_df = train_df.iloc[:80]\\ntrain_dataset = HandScanDataset2(data_dir=training_data_dir, transform=train_transforms, device=torch.device(\"cpu\"))  # Use CPU for initial loading\\n\\n# Cache the training dataset in memory\\ncached_train_dataset = CachedDataset(train_dataset)\\n\\n# Create a DataLoader for the cached training dataset\\ntrain_loader = DataLoader(cached_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\\n\\n# Select a few subjects for validation (e.g., the first 20 subjects)\\nvalid_subset_df = valid_df.iloc[:20]\\nvalid_dataset = HandScanDataset2(data_dir=training_data_dir, transform=test_transforms, device=torch.device(\"cpu\"))  # Use CPU for initial loading\\n\\n# Cache the validation dataset in memory\\ncached_valid_dataset = CachedDataset(valid_dataset)\\n\\n# Create a DataLoader for the cached validation dataset\\ntest_loader = DataLoader(cached_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\\n\\n# After creating the DataLoader, move the data to the GPU\\nfor i in range(len(cached_train_dataset)):\\n    cached_train_dataset.data[i] = cached_train_dataset.data[i].to(torch.device(\"cuda\"))\\n\\nfor i in range(len(cached_valid_dataset)):\\n    cached_valid_dataset.data[i] = cached_valid_dataset.data[i].to(torch.device(\"cuda\"))\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Batch size and number of workers\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "\n",
    "# Select a few subjects for training (e.g., the first 80 subjects)\n",
    "train_subset_df = train_df.iloc[:80]\n",
    "train_dataset = HandScanDataset2(data_dir=training_data_dir, transform=train_transforms, device=torch.device(\"cpu\"))  # Use CPU for initial loading\n",
    "\n",
    "# Cache the training dataset in memory\n",
    "cached_train_dataset = CachedDataset(train_dataset)\n",
    "\n",
    "# Create a DataLoader for the cached training dataset\n",
    "train_loader = DataLoader(cached_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Select a few subjects for validation (e.g., the first 20 subjects)\n",
    "valid_subset_df = valid_df.iloc[:20]\n",
    "valid_dataset = HandScanDataset2(data_dir=training_data_dir, transform=test_transforms, device=torch.device(\"cpu\"))  # Use CPU for initial loading\n",
    "\n",
    "# Cache the validation dataset in memory\n",
    "cached_valid_dataset = CachedDataset(valid_dataset)\n",
    "\n",
    "# Create a DataLoader for the cached validation dataset\n",
    "test_loader = DataLoader(cached_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# After creating the DataLoader, move the data to the GPU\n",
    "for i in range(len(cached_train_dataset)):\n",
    "    cached_train_dataset.data[i] = cached_train_dataset.data[i].to(torch.device(\"cuda\"))\n",
    "\n",
    "for i in range(len(cached_valid_dataset)):\n",
    "    cached_valid_dataset.data[i] = cached_valid_dataset.data[i].to(torch.device(\"cuda\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.activation_count = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.activation_count += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Instantiate EarlyStopper\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train(dict_key_for_training, max_epochs=10, learning_rate=1e-3):\\n    model = AutoEncoder(\\n        spatial_dims=3,\\n        in_channels=1,\\n        out_channels=1,\\n        channels=(4, 8, 16, 32),\\n        strides=(2, 2, 2, 2),\\n    ).to(device)\\n\\n    # Create loss function and optimizer\\n    loss_function = torch.nn.MSELoss()\\n    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\\n\\n    epoch_loss_values = []\\n\\n    t = trange(max_epochs, desc=f\"{dict_key_for_training} -- epoch 0, avg loss: inf\", leave=True)\\n    for epoch in t:\\n        model.train()\\n        epoch_loss = 0\\n        step = 0\\n        for batch_data in train_loader:\\n            step += 1\\n\\n            # Check if the key exists in batch_data\\n            if dict_key_for_training not in batch_data:\\n                raise KeyError(f\"Key \\'{dict_key_for_training}\\' not found in batch data. Available keys: {batch_data.keys()}\")\\n\\n            inputs = batch_data[dict_key_for_training].squeeze(dim=2).to(device)\\n            optimizer.zero_grad()\\n            outputs = model(inputs)\\n\\n            # Ensure the \"orig\" key is present\\n            if \"orig\" not in batch_data:\\n                raise KeyError(\"\\'orig\\' key not found in batch data. Available keys: {batch_data.keys()}\")\\n            loss = loss_function(outputs, batch_data[\"orig\"].to(device))\\n            loss.backward()\\n            optimizer.step()\\n            epoch_loss += loss.item()\\n        \\n        epoch_loss /= step\\n        epoch_loss_values.append(epoch_loss)\\n        t.set_description(\\n            f\"{dict_key_for_training} -- epoch {epoch + 1}\" + f\", average loss: {epoch_loss:.4f}\"\\n        )\\n    \\n    return model, epoch_loss_values\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def train(dict_key_for_training, max_epochs=10, learning_rate=1e-3):\n",
    "    model = AutoEncoder(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=(4, 8, 16, 32),\n",
    "        strides=(2, 2, 2, 2),\n",
    "    ).to(device)\n",
    "\n",
    "    # Create loss function and optimizer\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    t = trange(max_epochs, desc=f\"{dict_key_for_training} -- epoch 0, avg loss: inf\", leave=True)\n",
    "    for epoch in t:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "\n",
    "            # Check if the key exists in batch_data\n",
    "            if dict_key_for_training not in batch_data:\n",
    "                raise KeyError(f\"Key '{dict_key_for_training}' not found in batch data. Available keys: {batch_data.keys()}\")\n",
    "\n",
    "            inputs = batch_data[dict_key_for_training].squeeze(dim=2).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Ensure the \"orig\" key is present\n",
    "            if \"orig\" not in batch_data:\n",
    "                raise KeyError(\"'orig' key not found in batch data. Available keys: {batch_data.keys()}\")\n",
    "            loss = loss_function(outputs, batch_data[\"orig\"].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        t.set_description(\n",
    "            f\"{dict_key_for_training} -- epoch {epoch + 1}\" + f\", average loss: {epoch_loss:.4f}\"\n",
    "        )\n",
    "    \n",
    "    return model, epoch_loss_values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_dir = \"./output\"\\nlog_dir = \"./logs\"\\n\\n# Set up basic logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(\"swin_transformer_3d\")\\n\\n# Create a TensorBoard writer\\nwriter = SummaryWriter(log_dir=log_dir)\\n\\n# Contrastive Loss Function\\ndef contrastive_loss(zsim, zlabel, alpha=0.05):\\n    bce_loss = nn.BCEWithLogitsLoss()\\n    return alpha * bce_loss(zsim, zlabel)\\n\\ndef train(dict_key_for_training, max_epochs=10, learning_rate=1e-3, alpha=0.05, patience=5):\\n    model = AutoEncoder(\\n        spatial_dims=3,\\n        in_channels=1,\\n        out_channels=1,\\n        channels=(4, 8, 16, 32),\\n        strides=(2, 2, 2, 2),\\n    ).to(device)\\n\\n    loss_function = nn.MSELoss()\\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n    early_stopper = EarlyStopper(patience=patience)\\n\\n    epoch_loss_values = []\\n\\n    for epoch in range(max_epochs):\\n        model.train()\\n        epoch_loss = 0\\n        step = 0\\n        for batch_data in train_loader:\\n            step += 1\\n\\n            if dict_key_for_training not in batch_data:\\n                raise KeyError(f\"Key \\'{dict_key_for_training}\\' not found in batch data. Available keys: {batch_data.keys()}\")\\n\\n            inputs = batch_data[dict_key_for_training].squeeze(dim=2).to(device)\\n            optimizer.zero_grad()\\n            outputs = model(inputs)\\n\\n            if \"orig\" not in batch_data:\\n                raise KeyError(\"\\'orig\\' key not found in batch data. Available keys: {batch_data.keys()}\")\\n\\n            loss = loss_function(outputs, batch_data[\"orig\"].to(device))\\n            loss.backward()\\n            optimizer.step()\\n            epoch_loss += loss.item()\\n\\n        epoch_loss /= step\\n        epoch_loss_values.append(epoch_loss)\\n\\n        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\\n\\n        # Validation loop\\n        total_val_loss = 0.0\\n        model.eval()\\n        with torch.no_grad():\\n            for batch_data in test_loader:\\n                inputs = batch_data[dict_key_for_training].squeeze(dim=2).to(device)\\n                outputs = model(inputs)\\n\\n                loss_L1 = loss_function(outputs, batch_data[\"orig\"].to(device))\\n                total_val_loss += loss_L1.item()\\n\\n        avg_val_loss = total_val_loss / len(test_loader)\\n        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\\n\\n        # Early stopping check\\n        if early_stopper.early_stop(avg_val_loss):\\n            if early_stopper.activation_count == 1:\\n                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\\n                opt_model_filepath = f\"{output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\"\\n                torch.save(model.state_dict(), opt_model_filepath)\\n\\n        # Logging to TensorBoard\\n        writer.add_scalar(\\'Loss/train\\', epoch_loss, epoch)\\n        writer.add_scalar(\\'Loss/validation\\', avg_val_loss, epoch)\\n\\n    return model, epoch_loss_values\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''output_dir = \"./output\"\n",
    "log_dir = \"./logs\"\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Contrastive Loss Function\n",
    "def contrastive_loss(zsim, zlabel, alpha=0.05):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    return alpha * bce_loss(zsim, zlabel)\n",
    "\n",
    "def train(dict_key_for_training, max_epochs=10, learning_rate=1e-3, alpha=0.05, patience=5):\n",
    "    model = AutoEncoder(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=(4, 8, 16, 32),\n",
    "        strides=(2, 2, 2, 2),\n",
    "    ).to(device)\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "\n",
    "            if dict_key_for_training not in batch_data:\n",
    "                raise KeyError(f\"Key '{dict_key_for_training}' not found in batch data. Available keys: {batch_data.keys()}\")\n",
    "\n",
    "            inputs = batch_data[dict_key_for_training].squeeze(dim=2).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if \"orig\" not in batch_data:\n",
    "                raise KeyError(\"'orig' key not found in batch data. Available keys: {batch_data.keys()}\")\n",
    "\n",
    "            loss = loss_function(outputs, batch_data[\"orig\"].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        total_val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                inputs = batch_data[dict_key_for_training].squeeze(dim=2).to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss_L1 = loss_function(outputs, batch_data[\"orig\"].to(device))\n",
    "                total_val_loss += loss_L1.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper.early_stop(avg_val_loss):\n",
    "            if early_stopper.activation_count == 1:\n",
    "                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "                opt_model_filepath = f\"{output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "                torch.save(model.state_dict(), opt_model_filepath)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "\n",
    "    return model, epoch_loss_values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import logging\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.tensorboard import SummaryWriter\\nfrom monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\\nfrom monai.data import DataLoader\\nfrom monai.utils import set_determinism\\n\\n# Initialize paths\\noutput_dir = \"./output\"\\nlog_dir = \"./logs\"\\n\\n# Set up basic logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(\"swin_transformer_3d\")\\n\\n# Create a TensorBoard writer\\nwriter = SummaryWriter(log_dir=log_dir)\\n\\n# Define the Contrastive Loss Function\\ndef contrastive_loss(zsim, zlabel, alpha=0.05):\\n    bce_loss = nn.BCEWithLogitsLoss()\\n    return alpha * bce_loss(zsim, zlabel)\\n\\n\\n# Training function using SwinUNETR\\ndef train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5):\\n    # Initialize the SwinUNETR model\\n    print(\\'initilaise model\\')\\n    model = SwinUNETR(\\n        img_size=(4, 512, 352), \\n        in_channels=1,\\n        out_channels=1,\\n        feature_size=48,\\n        spatial_dims=3,\\n        window_size=(1, 7, 7)\\n    ).to(device)\\n\\n    # Define the loss function and optimizer\\n    \\n    loss_function = nn.MSELoss()\\n    print(\\'defined loss\\')\\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n    print(\\'defined optimizer\\')\\n    early_stopper = EarlyStopper(patience=patience)\\n\\n    epoch_loss_values = []\\n\\n    for epoch in range(max_epochs):\\n        model.train()\\n        epoch_loss = 0\\n        step = 0\\n        for batch_data in train_loader:\\n            step += 1\\n\\n            inputs = batch_data[\"im\"].to(device)\\n            print(inputs.shape)\\n            optimizer.zero_grad()\\n            outputs = model(inputs)\\n            print(outputs.shape)\\n\\n            loss = loss_function(outputs, batch_data[\"orig\"].to(device))\\n            print(\"calculated loss\")\\n            loss.backward()\\n            optimizer.step()\\n            epoch_loss += loss.item()\\n\\n        epoch_loss /= step\\n        epoch_loss_values.append(epoch_loss)\\n\\n        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\\n\\n        # Validation loop\\n        total_val_loss = 0.0\\n        model.eval()\\n        with torch.no_grad():\\n            for batch_data in test_loader:\\n                inputs = batch_data[\"im\"].to(device)\\n                outputs = model(inputs)\\n\\n                loss_L1 = loss_function(outputs, batch_data[\"orig\"].to(device))\\n                total_val_loss += loss_L1.item()\\n\\n        avg_val_loss = total_val_loss / len(test_loader)\\n        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\\n\\n        # Early stopping check\\n        if early_stopper.early_stop(avg_val_loss):\\n            # Save model parameters if early stopping criteria are met for the first time\\n            if early_stopper.activation_count == 1:\\n                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\\n                # Save network parameters and losses\\n                opt_model_filepath = f\"{output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\"\\n                torch.save(model.state_dict(), opt_model_filepath)\\n\\n        # Logging to TensorBoard\\n        writer.add_scalar(\\'Loss/train\\', epoch_loss, epoch)\\n        writer.add_scalar(\\'Loss/validation\\', avg_val_loss, epoch)\\n\\n    return model, epoch_loss_values\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Initialize paths\n",
    "output_dir = \"./output\"\n",
    "log_dir = \"./logs\"\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Define the Contrastive Loss Function\n",
    "def contrastive_loss(zsim, zlabel, alpha=0.05):\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    return alpha * bce_loss(zsim, zlabel)\n",
    "\n",
    "\n",
    "# Training function using SwinUNETR\n",
    "def train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5):\n",
    "    # Initialize the SwinUNETR model\n",
    "    print('initilaise model')\n",
    "    model = SwinUNETR(\n",
    "        img_size=(4, 512, 352), \n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        feature_size=48,\n",
    "        spatial_dims=3,\n",
    "        window_size=(1, 7, 7)\n",
    "    ).to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    \n",
    "    loss_function = nn.MSELoss()\n",
    "    print('defined loss')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print('defined optimizer')\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "\n",
    "            inputs = batch_data[\"im\"].to(device)\n",
    "            print(inputs.shape)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            print(outputs.shape)\n",
    "\n",
    "            loss = loss_function(outputs, batch_data[\"orig\"].to(device))\n",
    "            print(\"calculated loss\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        total_val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                inputs = batch_data[\"im\"].to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss_L1 = loss_function(outputs, batch_data[\"orig\"].to(device))\n",
    "                total_val_loss += loss_L1.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper.early_stop(avg_val_loss):\n",
    "            # Save model parameters if early stopping criteria are met for the first time\n",
    "            if early_stopper.activation_count == 1:\n",
    "                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "                # Save network parameters and losses\n",
    "                opt_model_filepath = f\"{output_dir}/optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "                torch.save(model.state_dict(), opt_model_filepath)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "\n",
    "    return model, epoch_loss_values\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Set hyperparameters\\nmax_epochs = 10\\nlearning_rate = 1e-3\\npatience = 5\\nmodels = []\\nepoch_losses = []\\n\\n# Run the training function\\nmodel, epoch_loss = train(train_loader, test_loader, max_epochs=max_epochs, learning_rate=learning_rate, patience=patience)\\n\\n# Store the trained model and loss values\\nmodels.append(model)\\nepoch_losses.append(epoch_loss)\\n# The model and epoch_loss are returned, and you can save the model or analyze the loss further'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Set hyperparameters\n",
    "max_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "patience = 5\n",
    "models = []\n",
    "epoch_losses = []\n",
    "\n",
    "# Run the training function\n",
    "model, epoch_loss = train(train_loader, test_loader, max_epochs=max_epochs, learning_rate=learning_rate, patience=patience)\n",
    "\n",
    "# Store the trained model and loss values\n",
    "models.append(model)\n",
    "epoch_losses.append(epoch_loss)\n",
    "# The model and epoch_loss are returned, and you can save the model or analyze the loss further'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_epochs = 10\\nlearning_rate = 0.00005\\nalpha = 0.05\\npatience = 5  # Early stopping patience\\ntraining_types = [\"orig\", \"gaus\", \"s&p\"]\\nmodels = []\\nepoch_losses = []\\n\\nfor training_type in training_types:\\n    print(f\"Training with {training_type} data...\")\\n\\n    # Call the train function\\n    model, epoch_loss = train(\\n        dict_key_for_training=training_type,\\n        train_loader=train_loader,\\n        test_loader=test_loader,\\n        max_epochs=max_epochs,\\n        learning_rate=learning_rate,\\n        alpha=alpha,\\n        patience=patience\\n    )\\n\\n    # Store the trained model and loss values\\n    models.append(model)\\n    epoch_losses.append(epoch_loss)\\n\\n    print(f\"Finished training with {training_type} data.\")'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''max_epochs = 10\n",
    "learning_rate = 0.00005\n",
    "alpha = 0.05\n",
    "patience = 5  # Early stopping patience\n",
    "training_types = [\"orig\", \"gaus\", \"s&p\"]\n",
    "models = []\n",
    "epoch_losses = []\n",
    "\n",
    "for training_type in training_types:\n",
    "    print(f\"Training with {training_type} data...\")\n",
    "\n",
    "    # Call the train function\n",
    "    model, epoch_loss = train(\n",
    "        dict_key_for_training=training_type,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        max_epochs=max_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        alpha=alpha,\n",
    "        patience=patience\n",
    "    )\n",
    "\n",
    "    # Store the trained model and loss values\n",
    "    models.append(model)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    print(f\"Finished training with {training_type} data.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_epochs = 1000\\nlearning_rate = 1e-3\\nalpha = 0.05\\npatience = 5  # Early stopping patience\\ntraining_types = [\"orig\", \"gaus\", \"s&p\"]\\nmodels = []\\nepoch_losses = []\\n\\nfor training_type in training_types:\\n    print(f\"Training with {training_type} data...\")\\n\\n    # Call the train function\\n    model, epoch_loss = train(\\n        dict_key_for_training=training_type,\\n        max_epochs=max_epochs,\\n        learning_rate=learning_rate,\\n        alpha=alpha,\\n        patience=patience\\n    )\\n\\n    # Store the trained model and loss values\\n    models.append(model)\\n    epoch_losses.append(epoch_loss)\\n\\n    print(f\"Finished training with {training_type} data.\")\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''max_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "alpha = 0.05\n",
    "patience = 5  # Early stopping patience\n",
    "training_types = [\"orig\", \"gaus\", \"s&p\"]\n",
    "models = []\n",
    "epoch_losses = []\n",
    "\n",
    "for training_type in training_types:\n",
    "    print(f\"Training with {training_type} data...\")\n",
    "\n",
    "    # Call the train function\n",
    "    model, epoch_loss = train(\n",
    "        dict_key_for_training=training_type,\n",
    "        max_epochs=max_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        alpha=alpha,\n",
    "        patience=patience\n",
    "    )\n",
    "\n",
    "    # Store the trained model and loss values\n",
    "    models.append(model)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    print(f\"Finished training with {training_type} data.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_epochs = 10\\ntraining_types = [\"orig\", \"gaus\", \"s&p\"]\\nmodels = []\\nepoch_losses = []\\nfor training_type in training_types:\\n    model, epoch_loss = train(training_type, max_epochs=max_epochs)\\n    models.append(model)\\n    epoch_losses.append(epoch_loss)'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''max_epochs = 10\n",
    "training_types = [\"orig\", \"gaus\", \"s&p\"]\n",
    "models = []\n",
    "epoch_losses = []\n",
    "for training_type in training_types:\n",
    "    model, epoch_loss = train(training_type, max_epochs=max_epochs)\n",
    "    models.append(model)\n",
    "    epoch_losses.append(epoch_loss)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport matplotlib.pyplot as plt\\n\\n# Function to get a single image from the dataset\\ndef get_single_im(ds):\\n    loader = torch.utils.data.DataLoader(ds, batch_size=1, num_workers=0, shuffle=True)\\n    itera = iter(loader)\\n    return next(itera)\\n\\n# Fetch a single image and its degraded versions\\ndata = get_single_im(train_ds)\\n\\n# Assuming you want to visualize the original and its degraded versions\\n# (e.g., \"orig\", \"gaus\", \"s&p\" keys in the data dictionary)\\nfor key in [\"orig\", \"gaus\", \"s&p\"]:\\n    im = data[key].squeeze()  # Remove any singleton dimensions, e.g., [1, 1, 512, 384] -> [512, 384]\\n    \\n    if im.ndim == 2:  # If it\\'s a 2D image, just plot it\\n        plt.figure()\\n        plt.imshow(im, cmap=\"gray\")\\n        plt.title(f\"{key}\")\\n        plt.show()\\n    \\n    elif im.ndim == 3:  # If it\\'s 3D (multi-channel), plot each channel separately\\n        for i in range(im.shape[0]):\\n            plt.figure()\\n            plt.imshow(im[i], cmap=\"gray\")\\n            plt.title(f\"{key} - Channel {i}\")\\n            plt.show()\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to get a single image from the dataset\n",
    "def get_single_im(ds):\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=1, num_workers=0, shuffle=True)\n",
    "    itera = iter(loader)\n",
    "    return next(itera)\n",
    "\n",
    "# Fetch a single image and its degraded versions\n",
    "data = get_single_im(train_ds)\n",
    "\n",
    "# Assuming you want to visualize the original and its degraded versions\n",
    "# (e.g., \"orig\", \"gaus\", \"s&p\" keys in the data dictionary)\n",
    "for key in [\"orig\", \"gaus\", \"s&p\"]:\n",
    "    im = data[key].squeeze()  # Remove any singleton dimensions, e.g., [1, 1, 512, 384] -> [512, 384]\n",
    "    \n",
    "    if im.ndim == 2:  # If it's a 2D image, just plot it\n",
    "        plt.figure()\n",
    "        plt.imshow(im, cmap=\"gray\")\n",
    "        plt.title(f\"{key}\")\n",
    "        plt.show()\n",
    "    \n",
    "    elif im.ndim == 3:  # If it's 3D (multi-channel), plot each channel separately\n",
    "        for i in range(im.shape[0]):\n",
    "            plt.figure()\n",
    "            plt.imshow(im[i], cmap=\"gray\")\n",
    "            plt.title(f\"{key} - Channel {i}\")\n",
    "            plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Get image original and its degraded versions\\ndef get_single_im(ds):\\n    loader = torch.utils.data.DataLoader(train_ds, batch_size=1, num_workers=0, shuffle=True)\\n    itera = iter(loader)\\n    return next(itera)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Get image original and its degraded versions\n",
    "def get_single_im(ds):\n",
    "    loader = torch.utils.data.DataLoader(train_ds, batch_size=1, num_workers=0, shuffle=True)\n",
    "    itera = iter(loader)\n",
    "    return next(itera)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(im.shape[0]):\\n    plt.figure()\\n    plt.imshow(im[i], cmap=\"gray\")\\n    plt.title(f\"Channel {i}\")\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(im.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(im[i], cmap=\"gray\")\n",
    "    plt.title(f\"Channel {i}\")\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-29 13:49:46,464 - INFO - Expected md5 is None, skip md5 check for file C:\\Users\\scmmw\\OneDrive - University of Leeds\\ssl_pretrained_weights.\n",
      "2024-08-29 13:49:46,464 - INFO - File exists: C:\\Users\\scmmw\\OneDrive - University of Leeds\\ssl_pretrained_weights, skipped downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-29 13:49:47,478 - INFO - 'dst' model updated: 159 of 159 variables.\n"
     ]
    }
   ],
   "source": [
    "from monai.apps import download_url\n",
    "pre_training_weights_path='C:/Users/scmmw/OneDrive - University of Leeds/ssl_pretrained_weights'\n",
    "swin_model = SwinUNETR(\n",
    "    img_size=(32, 96, 96),\n",
    "    in_channels=1,\n",
    "    out_channels=1,  \n",
    "    feature_size=48,  # Match the pre-trained model\n",
    "    depths=(2, 2, 2, 2),  # Match the pre-trained model\n",
    "    num_heads=(3, 6, 12, 24),  # Match the pre-trained model\n",
    "    spatial_dims=3,\n",
    "    use_checkpoint=True  # You can keep this as True if you need it\n",
    ")\n",
    "swin_model = swin_model.to(torch.float32)\n",
    "swin_model = swin_model.to(device)\n",
    "\n",
    "resource = (\n",
    "    \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n",
    ")\n",
    "download_url(resource, pre_training_weights_path)\n",
    "ssl_weights = torch.load(pre_training_weights_path)[\"model\"]\n",
    "\n",
    "dst_dict, loaded, not_loaded = copy_model_state(swin_model, ssl_weights, filter_func=filter_swinunetr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.norm1.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.norm1.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.attn.relative_position_bias_table\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.attn.qkv.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.attn.qkv.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.attn.proj.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.attn.proj.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.norm2.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.norm2.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.mlp.linear1.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.mlp.linear1.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.mlp.linear2.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.0.mlp.linear2.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.norm1.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.norm1.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.attn.relative_position_bias_table\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.attn.qkv.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.attn.qkv.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.attn.proj.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.attn.proj.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.norm2.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.norm2.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.mlp.linear1.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.mlp.linear1.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.mlp.linear2.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.blocks.1.mlp.linear2.bias\n",
      "Fine-tuning layer: swinViT.layers3.0.downsample.reduction.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.downsample.norm.weight\n",
      "Fine-tuning layer: swinViT.layers3.0.downsample.norm.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.norm1.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.norm1.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.attn.relative_position_bias_table\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.attn.qkv.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.attn.qkv.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.attn.proj.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.attn.proj.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.norm2.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.norm2.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.mlp.linear1.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.mlp.linear1.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.mlp.linear2.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.0.mlp.linear2.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.norm1.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.norm1.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.attn.relative_position_bias_table\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.attn.qkv.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.attn.qkv.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.attn.proj.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.attn.proj.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.norm2.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.norm2.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.mlp.linear1.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.mlp.linear1.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.mlp.linear2.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.blocks.1.mlp.linear2.bias\n",
      "Fine-tuning layer: swinViT.layers4.0.downsample.reduction.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.downsample.norm.weight\n",
      "Fine-tuning layer: swinViT.layers4.0.downsample.norm.bias\n",
      "Fine-tuning layer: encoder4.layer.conv1.conv.weight\n",
      "Fine-tuning layer: encoder4.layer.conv2.conv.weight\n",
      "Fine-tuning layer: encoder10.layer.conv1.conv.weight\n",
      "Fine-tuning layer: encoder10.layer.conv2.conv.weight\n",
      "Fine-tuning layer: decoder5.transp_conv.conv.weight\n",
      "Fine-tuning layer: decoder5.conv_block.conv1.conv.weight\n",
      "Fine-tuning layer: decoder5.conv_block.conv2.conv.weight\n",
      "Fine-tuning layer: decoder5.conv_block.conv3.conv.weight\n",
      "Fine-tuning layer: decoder4.transp_conv.conv.weight\n",
      "Fine-tuning layer: decoder4.conv_block.conv1.conv.weight\n",
      "Fine-tuning layer: decoder4.conv_block.conv2.conv.weight\n",
      "Fine-tuning layer: decoder4.conv_block.conv3.conv.weight\n",
      "Fine-tuning layer: out.conv.conv.weight\n",
      "Fine-tuning layer: out.conv.conv.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze specific layers if you don't want them to be updated during fine-tuning\n",
    "for name, param in swin_model.named_parameters():\n",
    "    if \"swinViT.layers1\" in name or \"swinViT.layers2\" in name:\n",
    "        param.requires_grad = False  # Freeze these layers\n",
    "\n",
    "\n",
    "# Freeze specific layers if you don't want them to be updated during fine-tuning\n",
    "for name, param in swin_model.named_parameters():\n",
    "    if \"patch_embed\" in name:\n",
    "        param.requires_grad = False  # Freeze these layers\n",
    "\n",
    "\n",
    "for name, param in swin_model.named_parameters():\n",
    "    if 'encoder' in name and 'encoder4' not in name and 'encoder10' not in name:\n",
    "        param.requires_grad = False  # Freeze these layers\n",
    "\n",
    "# Freeze some parts of the decoder if needed\n",
    "for name, param in swin_model.named_parameters():\n",
    "     if 'decoder' in name and 'decoder4' not in name and 'decoder5' not in name:\n",
    "         param.requires_grad = False\n",
    "\n",
    "# Print which layers will be fine-tuned\n",
    "for name, param in swin_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Fine-tuning layer: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\scmmw\\\\OneDrive - University of Leeds\\\\Masters - 23-24\\\\Project\\\\results'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define path to where I will save results\n",
    "ResultsPath = r'C:\\Users\\scmmw\\OneDrive - University of Leeds\\Masters - 23-24\\Project\\results'\n",
    "os.makedirs(ResultsPath, exist_ok=True)\n",
    "ResultsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import logging\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.tensorboard import SummaryWriter\\nfrom monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\\nfrom monai.data import DataLoader\\nfrom monai.utils import set_determinism\\n\\n# Initialize paths\\n\\nlog_dir = ResultsPath\\n\\n# Set up basic logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(\"swin_transformer_3d\")\\n\\n# Create a TensorBoard writer\\nwriter = SummaryWriter(log_dir=log_dir)\\n\\n\\n# Training function using SwinUNETR\\ndef train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5, model=None):\\n    # Initialize the SwinUNETR model\\n    logger.info(\\'Initializing model\\')\\n\\n    # Define the loss function and optimizer\\n    loss_function = nn.MSELoss()\\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n    early_stopper = EarlyStopper(patience=patience)\\n\\n    epoch_loss_values = []\\n\\n    for epoch in range(max_epochs):\\n        epoch_start_time = time.time()\\n        model.train()\\n        epoch_loss = 0\\n        step = 0\\n\\n        # Progress bar for the epoch\\n        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{max_epochs}\", unit=\"batch\") as pbar:\\n            for batch_data in train_loader:\\n                batch_start_time = time.time()\\n                step += 1\\n\\n                inputs = batch_data[\"im\"].to(device)\\n                inputs = inputs.to(device)\\n                optimizer.zero_grad()\\n                outputs = model(inputs)\\n                outputs = outputs.to(device)\\n\\n                loss = loss_function(outputs, inputs)\\n                loss.backward()\\n                optimizer.step()\\n                epoch_loss += loss.item()\\n\\n                batch_time = time.time() - batch_start_time\\n                pbar.set_postfix({\"Batch Time\": f\"{batch_time:.4f} sec\"})\\n                pbar.update(1)\\n\\n        epoch_loss /= step\\n        epoch_loss_values.append(epoch_loss)\\n\\n        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\\n\\n        # Validation loop with progress bar\\n        total_val_loss = 0.0\\n        model.eval()\\n        with tqdm(total=len(test_loader), desc=\"Validation\", unit=\"batch\") as pbar:\\n            with torch.no_grad():\\n                for batch_data in test_loader:\\n                    inputs = batch_data[\"im\"]\\n                    inputs = inputs.to(device)\\n                    outputs = model(inputs)\\n                    outputs = outputs.to(device)\\n                    loss_L1 = loss_function(outputs, inputs)\\n                    total_val_loss += loss_L1.item()\\n                    pbar.update(1)\\n\\n        avg_val_loss = total_val_loss / len(test_loader)\\n        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\\n\\n        epoch_time = time.time() - epoch_start_time\\n        logger.info(f\"Epoch {epoch + 1} took {epoch_time:.4f} seconds\")\\n\\n        # Early stopping check\\n        if early_stopper.early_stop(avg_val_loss):\\n            # Save model parameters if early stopping criteria are met for the first time\\n            if early_stopper.counter == 1:\\n                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\\n                opt_model_filepath = f\"{ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\"\\n                torch.save(model.state_dict(), opt_model_filepath)\\n\\n        # Logging to TensorBoard\\n        writer.add_scalar(\\'Loss/train\\', epoch_loss, epoch)\\n        writer.add_scalar(\\'Loss/validation\\', avg_val_loss, epoch)\\n\\n    return model, epoch_loss_values'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Initialize paths\n",
    "\n",
    "log_dir = ResultsPath\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "# Training function using SwinUNETR\n",
    "def train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5, model=None):\n",
    "    # Initialize the SwinUNETR model\n",
    "    logger.info('Initializing model')\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        # Progress bar for the epoch\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{max_epochs}\", unit=\"batch\") as pbar:\n",
    "            for batch_data in train_loader:\n",
    "                batch_start_time = time.time()\n",
    "                step += 1\n",
    "\n",
    "                inputs = batch_data[\"im\"].to(device)\n",
    "                inputs = inputs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                loss = loss_function(outputs, inputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                pbar.set_postfix({\"Batch Time\": f\"{batch_time:.4f} sec\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop with progress bar\n",
    "        total_val_loss = 0.0\n",
    "        model.eval()\n",
    "        with tqdm(total=len(test_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for batch_data in test_loader:\n",
    "                    inputs = batch_data[\"im\"]\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.to(device)\n",
    "                    loss_L1 = loss_function(outputs, inputs)\n",
    "                    total_val_loss += loss_L1.item()\n",
    "                    pbar.update(1)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        logger.info(f\"Epoch {epoch + 1} took {epoch_time:.4f} seconds\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper.early_stop(avg_val_loss):\n",
    "            # Save model parameters if early stopping criteria are met for the first time\n",
    "            if early_stopper.counter == 1:\n",
    "                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "                opt_model_filepath = f\"{ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "                torch.save(model.state_dict(), opt_model_filepath)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "\n",
    "    return model, epoch_loss_values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "from torch.amp import autocast, GradScaler  # Use the updated GradScaler from torch.amp\n",
    "\n",
    "# Initialize paths\n",
    "log_dir = ResultsPath\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Enable cuDNN auto-tuner to find the best algorithm\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Training function using SwinUNETR\n",
    "def train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5, model=None):\n",
    "    # Initialize the SwinUNETR model\n",
    "    logger.info('Initializing model')\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    early_stopper = EarlyStopper(patience=patience, min_delta=0.003)\n",
    "\n",
    "    # Initialize GradScaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        # Progress bar for the epoch\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{max_epochs}\", unit=\"batch\") as pbar:\n",
    "            for batch_data in train_loader:\n",
    "                batch_start_time = time.time()\n",
    "                step += 1\n",
    "\n",
    "                inputs = batch_data[\"im\"].to(torch.float32).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Use autocast for mixed precision during the forward pass\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_function(outputs, inputs)\n",
    "                \n",
    "                # Scale the loss and perform backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                pbar.set_postfix({\"Batch Time\": f\"{batch_time:.4f} sec\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop with progress bar\n",
    "        total_val_loss = 0.0\n",
    "        model.eval()\n",
    "        with tqdm(total=len(test_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for batch_data in test_loader:\n",
    "                    inputs = batch_data[\"im\"].to(torch.float32).to(device)\n",
    "\n",
    "                    # Use autocast for mixed precision during the forward pass in validation\n",
    "                    with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                        outputs = model(inputs)\n",
    "                        loss_L1 = loss_function(outputs, inputs)\n",
    "                        total_val_loss += loss_L1.item()\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        logger.info(f\"Epoch {epoch + 1} took {epoch_time:.4f} seconds\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper.early_stop(avg_val_loss):\n",
    "            # Save model parameters if early stopping criteria are met for the first time\n",
    "            if early_stopper.counter == 1:\n",
    "                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "                opt_model_filepath = f\"{ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "                torch.save(model.state_dict(), opt_model_filepath)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "\n",
    "    return model, epoch_loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curve(epoch_losses, early_stop_epoch=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss curves and mark the early stopping point.\n",
    "\n",
    "    Args:\n",
    "        epoch_losses (list of tuples): A list where each element is a tuple containing (train_loss, val_loss) for each epoch.\n",
    "        early_stop_epoch (int, optional): The epoch number where early stopping occurred. Defaults to None.\n",
    "        save_path (str, optional): Path to save the plot image. Defaults to None.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(epoch_losses) + 1)\n",
    "    train_losses = [loss[0] for loss in epoch_losses]\n",
    "    val_losses = [loss[1] for loss in epoch_losses]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
    "\n",
    "    if early_stop_epoch is not None:\n",
    "        plt.axvline(x=early_stop_epoch, color='r', linestyle='--', label=f'Early Stopping at Epoch {early_stop_epoch}')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import logging\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.tensorboard import SummaryWriter\\nfrom monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\\nfrom monai.data import DataLoader\\nfrom monai.utils import set_determinism\\n\\n# Initialize paths\\n\\nlog_dir = ResultsPath\\n\\n# Set up basic logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(\"swin_transformer_3d\")\\n\\n# Create a TensorBoard writer\\nwriter = SummaryWriter(log_dir=log_dir)\\n\\n# Training function using SwinUNETR\\ndef train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5, model=None):\\n    # Initialize the SwinUNETR model\\n    print(device)\\n    logger.info(\\'Initializing model\\')\\n\\n    # Ensure the model is in float32 precision\\n    model = model.to(device)\\n    model = model.to(torch.float32)\\n\\n    # Define the loss function and optimizer\\n    loss_function = nn.MSELoss()\\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\\n    early_stopper = EarlyStopper(patience=patience)\\n\\n    epoch_loss_values = []\\n\\n    for epoch in range(max_epochs):\\n        epoch_start_time = time.time()\\n        model.train()\\n        epoch_loss = 0\\n        step = 0\\n\\n        # Progress bar for the epoch\\n        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{max_epochs}\", unit=\"batch\") as pbar:\\n            for batch_data in train_loader:\\n                batch_start_time = time.time()\\n                step += 1\\n\\n                # Ensure the inputs are in float32 precision\\n                inputs = batch_data[\"im\"]\\n                inputs = inputs.to(torch.float32)\\n                inputs = inputs.to(device)\\n                optimizer.zero_grad()\\n\\n                # Forward pass\\n                outputs = model(inputs)\\n                outputs = outputs.to(torch.float32)\\n                outputs = outputs.to(device)\\n\\n                # Compute loss\\n                loss = loss_function(outputs, inputs.to(device))\\n                \\n                # Backward pass and optimize\\n                loss.backward()\\n                optimizer.step()\\n\\n                epoch_loss += loss.item()\\n\\n                batch_time = time.time() - batch_start_time\\n                pbar.set_postfix({\"Batch Time\": f\"{batch_time:.4f} sec\"})\\n                pbar.update(1)\\n\\n        epoch_loss /= step\\n        epoch_loss_values.append(epoch_loss)\\n\\n        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\\n\\n        # Validation loop with progress bar\\n        total_val_loss = 0.0\\n        model.eval()\\n        with tqdm(total=len(test_loader), desc=\"Validation\", unit=\"batch\") as pbar:\\n            with torch.no_grad():\\n                for batch_data in test_loader:\\n                    # Ensure the inputs are in float32 precision\\n                    inputs = batch_data[\"im\"]\\n                    inputs = inputs.to(torch.float32)\\n                    inputs = inputs.to(device)\\n\\n                    # Forward pass\\n                    outputs = model(inputs)\\n                    outputs = outputs.to(torch.float32)\\n                    outputs = output.to(device)\\n\\n                    # Compute validation loss\\n                    loss_L1 = loss_function(outputs, inputs.to(device))\\n                    total_val_loss += loss_L1.item()\\n                    \\n                    pbar.update(1)\\n\\n        avg_val_loss = total_val_loss / len(test_loader)\\n        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\\n\\n        epoch_time = time.time() - epoch_start_time\\n        logger.info(f\"Epoch {epoch + 1} took {epoch_time:.4f} seconds\")\\n\\n        # Early stopping check\\n        if early_stopper.early_stop(avg_val_loss):\\n            # Save model parameters if early stopping criteria are met for the first time\\n            if early_stopper.counter == 1:\\n                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\\n                opt_model_filepath = f\"{ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\"\\n                torch.save(model.state_dict(), opt_model_filepath)\\n\\n        # Logging to TensorBoard\\n        writer.add_scalar(\\'Loss/train\\', epoch_loss, epoch)\\n        writer.add_scalar(\\'Loss/validation\\', avg_val_loss, epoch)\\n\\n    return model, epoch_loss_values'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.networks.nets import SwinUNETR  # Import SwinUNETR from MONAI\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Initialize paths\n",
    "\n",
    "log_dir = ResultsPath\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"swin_transformer_3d\")\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Training function using SwinUNETR\n",
    "def train(train_loader, test_loader, max_epochs=10, learning_rate=1e-3, patience=5, model=None):\n",
    "    # Initialize the SwinUNETR model\n",
    "    print(device)\n",
    "    logger.info('Initializing model')\n",
    "\n",
    "    # Ensure the model is in float32 precision\n",
    "    model = model.to(device)\n",
    "    model = model.to(torch.float32)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    early_stopper = EarlyStopper(patience=patience)\n",
    "\n",
    "    epoch_loss_values = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        # Progress bar for the epoch\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{max_epochs}\", unit=\"batch\") as pbar:\n",
    "            for batch_data in train_loader:\n",
    "                batch_start_time = time.time()\n",
    "                step += 1\n",
    "\n",
    "                # Ensure the inputs are in float32 precision\n",
    "                inputs = batch_data[\"im\"]\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                inputs = inputs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.to(torch.float32)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_function(outputs, inputs.to(device))\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                batch_time = time.time() - batch_start_time\n",
    "                pbar.set_postfix({\"Batch Time\": f\"{batch_time:.4f} sec\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop with progress bar\n",
    "        total_val_loss = 0.0\n",
    "        model.eval()\n",
    "        with tqdm(total=len(test_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for batch_data in test_loader:\n",
    "                    # Ensure the inputs are in float32 precision\n",
    "                    inputs = batch_data[\"im\"]\n",
    "                    inputs = inputs.to(torch.float32)\n",
    "                    inputs = inputs.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.to(torch.float32)\n",
    "                    outputs = output.to(device)\n",
    "\n",
    "                    # Compute validation loss\n",
    "                    loss_L1 = loss_function(outputs, inputs.to(device))\n",
    "                    total_val_loss += loss_L1.item()\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        logger.info(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        logger.info(f\"Epoch {epoch + 1} took {epoch_time:.4f} seconds\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopper.early_stop(avg_val_loss):\n",
    "            # Save model parameters if early stopping criteria are met for the first time\n",
    "            if early_stopper.counter == 1:\n",
    "                logger.info(f\"\\nEarly stopping criteria reached!\\nSaving model parameters to {ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\\n\")\n",
    "                opt_model_filepath = f\"{ResultsPath}/optimal_model_weights_epoch_{epoch + 1}.pth\"\n",
    "                torch.save(model.state_dict(), opt_model_filepath)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
    "\n",
    "    return model, epoch_loss_values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Initializing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80:   0%|          | 0/80 [00:00<?, ?batch/s]`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "Epoch 1/80: 100%|██████████| 80/80 [01:52<00:00,  1.41s/batch, Batch Time=0.1275 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 1/80, Loss: 0.1203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0670\n",
      "INFO:swin_transformer_3d:Epoch 1 took 137.6936 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80: 100%|██████████| 80/80 [01:47<00:00,  1.35s/batch, Batch Time=0.1405 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 2/80, Loss: 0.0528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0480\n",
      "INFO:swin_transformer_3d:Epoch 2 took 134.3170 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80: 100%|██████████| 80/80 [01:50<00:00,  1.38s/batch, Batch Time=0.1290 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 3/80, Loss: 0.0524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0397\n",
      "INFO:swin_transformer_3d:Epoch 3 took 133.3309 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.0967 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 4/80, Loss: 0.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0445\n",
      "INFO:swin_transformer_3d:Epoch 4 took 125.8413 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80: 100%|██████████| 80/80 [01:40<00:00,  1.25s/batch, Batch Time=0.1214 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 5/80, Loss: 0.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0537\n",
      "INFO:swin_transformer_3d:Epoch 5 took 122.3984 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80: 100%|██████████| 80/80 [01:42<00:00,  1.28s/batch, Batch Time=0.1348 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 6/80, Loss: 0.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0375\n",
      "INFO:swin_transformer_3d:Epoch 6 took 125.2883 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.1047 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 7/80, Loss: 0.0467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0513\n",
      "INFO:swin_transformer_3d:Epoch 7 took 126.9150 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.1265 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 8/80, Loss: 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0581\n",
      "INFO:swin_transformer_3d:Epoch 8 took 125.3338 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/80: 100%|██████████| 80/80 [01:44<00:00,  1.31s/batch, Batch Time=0.0775 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 9/80, Loss: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0406\n",
      "INFO:swin_transformer_3d:Epoch 9 took 128.8292 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/80: 100%|██████████| 80/80 [01:43<00:00,  1.30s/batch, Batch Time=0.0929 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 10/80, Loss: 0.0378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0362\n",
      "INFO:swin_transformer_3d:Epoch 10 took 128.3626 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/80: 100%|██████████| 80/80 [01:42<00:00,  1.28s/batch, Batch Time=0.1070 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 11/80, Loss: 0.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0388\n",
      "INFO:swin_transformer_3d:Epoch 11 took 126.2073 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/80: 100%|██████████| 80/80 [01:40<00:00,  1.26s/batch, Batch Time=0.1049 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 12/80, Loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0279\n",
      "INFO:swin_transformer_3d:Epoch 12 took 123.4772 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/80: 100%|██████████| 80/80 [01:47<00:00,  1.35s/batch, Batch Time=0.0991 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 13/80, Loss: 0.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0290\n",
      "INFO:swin_transformer_3d:Epoch 13 took 131.8976 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/80: 100%|██████████| 80/80 [01:51<00:00,  1.39s/batch, Batch Time=0.1521 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 14/80, Loss: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0282\n",
      "INFO:swin_transformer_3d:Epoch 14 took 135.2003 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/80: 100%|██████████| 80/80 [01:42<00:00,  1.29s/batch, Batch Time=0.0992 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 15/80, Loss: 0.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0266\n",
      "INFO:swin_transformer_3d:Epoch 15 took 125.5438 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1230 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 16/80, Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0338\n",
      "INFO:swin_transformer_3d:Epoch 16 took 129.2341 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/80: 100%|██████████| 80/80 [01:50<00:00,  1.38s/batch, Batch Time=0.1309 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 17/80, Loss: 0.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0315\n",
      "INFO:swin_transformer_3d:Epoch 17 took 133.1177 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/80: 100%|██████████| 80/80 [01:46<00:00,  1.33s/batch, Batch Time=0.1247 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 18/80, Loss: 0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0317\n",
      "INFO:swin_transformer_3d:Epoch 18 took 129.5636 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.1490 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 19/80, Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0253\n",
      "INFO:swin_transformer_3d:Epoch 19 took 124.4586 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/80: 100%|██████████| 80/80 [01:42<00:00,  1.28s/batch, Batch Time=0.1150 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 20/80, Loss: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0248\n",
      "INFO:swin_transformer_3d:Epoch 20 took 124.2166 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/80: 100%|██████████| 80/80 [01:44<00:00,  1.30s/batch, Batch Time=0.1186 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 21/80, Loss: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0261\n",
      "INFO:swin_transformer_3d:Epoch 21 took 127.6458 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1047 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 22/80, Loss: 0.0271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0264\n",
      "INFO:swin_transformer_3d:Epoch 22 took 130.7375 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/80: 100%|██████████| 80/80 [01:43<00:00,  1.30s/batch, Batch Time=0.1093 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 23/80, Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0254\n",
      "INFO:swin_transformer_3d:Epoch 23 took 127.2109 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/80: 100%|██████████| 80/80 [01:42<00:00,  1.28s/batch, Batch Time=0.1045 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 24/80, Loss: 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0218\n",
      "INFO:swin_transformer_3d:Epoch 24 took 124.9768 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/80: 100%|██████████| 80/80 [01:42<00:00,  1.28s/batch, Batch Time=0.1290 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 25/80, Loss: 0.0266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0217\n",
      "INFO:swin_transformer_3d:Epoch 25 took 125.0344 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1560 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 26/80, Loss: 0.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.31s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0234\n",
      "INFO:swin_transformer_3d:Epoch 26 took 133.4268 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/80: 100%|██████████| 80/80 [01:48<00:00,  1.36s/batch, Batch Time=0.1283 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 27/80, Loss: 0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0227\n",
      "INFO:swin_transformer_3d:Epoch 27 took 132.9590 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.0883 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 28/80, Loss: 0.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0254\n",
      "INFO:swin_transformer_3d:Epoch 28 took 123.7980 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/80: 100%|██████████| 80/80 [01:52<00:00,  1.41s/batch, Batch Time=0.1564 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 29/80, Loss: 0.0242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0265\n",
      "INFO:swin_transformer_3d:Epoch 29 took 137.1494 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/80: 100%|██████████| 80/80 [01:52<00:00,  1.40s/batch, Batch Time=0.1459 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 30/80, Loss: 0.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0212\n",
      "INFO:swin_transformer_3d:Epoch 30 took 136.5956 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/80: 100%|██████████| 80/80 [01:46<00:00,  1.33s/batch, Batch Time=0.1006 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 31/80, Loss: 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0216\n",
      "INFO:swin_transformer_3d:Epoch 31 took 130.3617 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/80: 100%|██████████| 80/80 [01:43<00:00,  1.29s/batch, Batch Time=0.1433 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 32/80, Loss: 0.0184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0208\n",
      "INFO:swin_transformer_3d:Epoch 32 took 125.4559 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/80: 100%|██████████| 80/80 [01:38<00:00,  1.23s/batch, Batch Time=0.1227 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 33/80, Loss: 0.0241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0169\n",
      "INFO:swin_transformer_3d:Epoch 33 took 121.0540 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/80: 100%|██████████| 80/80 [01:37<00:00,  1.22s/batch, Batch Time=0.1174 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 34/80, Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0226\n",
      "INFO:swin_transformer_3d:Epoch 34 took 121.0042 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.0912 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 35/80, Loss: 0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0255\n",
      "INFO:swin_transformer_3d:Epoch 35 took 130.3156 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.1327 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 36/80, Loss: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0195\n",
      "INFO:swin_transformer_3d:Epoch 36 took 124.2945 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/80: 100%|██████████| 80/80 [01:39<00:00,  1.24s/batch, Batch Time=0.0919 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 37/80, Loss: 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0335\n",
      "INFO:swin_transformer_3d:Epoch 37 took 122.3161 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1333 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 38/80, Loss: 0.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.27s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0229\n",
      "INFO:swin_transformer_3d:Epoch 38 took 132.5695 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/80: 100%|██████████| 80/80 [01:43<00:00,  1.30s/batch, Batch Time=0.1528 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 39/80, Loss: 0.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0243\n",
      "INFO:swin_transformer_3d:Epoch 39 took 125.8988 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/80: 100%|██████████| 80/80 [01:40<00:00,  1.25s/batch, Batch Time=0.1254 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 40/80, Loss: 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:21<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0189\n",
      "INFO:swin_transformer_3d:Epoch 40 took 122.2807 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.1088 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 41/80, Loss: 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.26s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0218\n",
      "INFO:swin_transformer_3d:Epoch 41 took 127.2716 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/80: 100%|██████████| 80/80 [01:59<00:00,  1.50s/batch, Batch Time=0.1338 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 42/80, Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0269\n",
      "INFO:swin_transformer_3d:Epoch 42 took 145.8505 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/80: 100%|██████████| 80/80 [01:47<00:00,  1.35s/batch, Batch Time=0.1089 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 43/80, Loss: 0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0209\n",
      "INFO:swin_transformer_3d:Epoch 43 took 130.1818 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/80: 100%|██████████| 80/80 [01:43<00:00,  1.30s/batch, Batch Time=0.1385 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 44/80, Loss: 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.11s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0151\n",
      "INFO:swin_transformer_3d:Epoch 44 took 125.9417 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/80: 100%|██████████| 80/80 [01:43<00:00,  1.30s/batch, Batch Time=0.1400 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 45/80, Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0200\n",
      "INFO:swin_transformer_3d:Epoch 45 took 125.9952 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/80: 100%|██████████| 80/80 [01:44<00:00,  1.30s/batch, Batch Time=0.1268 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 46/80, Loss: 0.0223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0204\n",
      "INFO:swin_transformer_3d:Epoch 46 took 126.0631 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/80: 100%|██████████| 80/80 [01:43<00:00,  1.30s/batch, Batch Time=0.1275 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 47/80, Loss: 0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0187\n",
      "INFO:swin_transformer_3d:Epoch 47 took 126.0616 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/80: 100%|██████████| 80/80 [01:40<00:00,  1.26s/batch, Batch Time=0.1367 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 48/80, Loss: 0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:47<00:00,  2.37s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0192\n",
      "INFO:swin_transformer_3d:Epoch 48 took 148.2457 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/80: 100%|██████████| 80/80 [12:23<00:00,  9.29s/batch, Batch Time=0.1474 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 49/80, Loss: 0.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [02:24<00:00,  7.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0168\n",
      "INFO:swin_transformer_3d:Epoch 49 took 887.1523 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1093 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 50/80, Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0204\n",
      "INFO:swin_transformer_3d:Epoch 50 took 131.5051 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/80: 100%|██████████| 80/80 [01:51<00:00,  1.39s/batch, Batch Time=0.1520 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 51/80, Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0219\n",
      "INFO:swin_transformer_3d:Epoch 51 took 134.6581 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/80: 100%|██████████| 80/80 [01:48<00:00,  1.35s/batch, Batch Time=0.1235 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 52/80, Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0236\n",
      "INFO:swin_transformer_3d:Epoch 52 took 131.6574 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/80: 100%|██████████| 80/80 [01:44<00:00,  1.31s/batch, Batch Time=0.1467 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 53/80, Loss: 0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0218\n",
      "INFO:swin_transformer_3d:Epoch 53 took 127.4757 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/80: 100%|██████████| 80/80 [01:44<00:00,  1.31s/batch, Batch Time=0.1367 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 54/80, Loss: 0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0237\n",
      "INFO:swin_transformer_3d:Epoch 54 took 127.5804 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/80: 100%|██████████| 80/80 [01:43<00:00,  1.29s/batch, Batch Time=0.1401 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 55/80, Loss: 0.0241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0195\n",
      "INFO:swin_transformer_3d:Epoch 55 took 125.6004 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/80: 100%|██████████| 80/80 [01:46<00:00,  1.33s/batch, Batch Time=0.1830 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 56/80, Loss: 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0187\n",
      "INFO:swin_transformer_3d:Epoch 56 took 131.2175 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/80: 100%|██████████| 80/80 [01:50<00:00,  1.38s/batch, Batch Time=0.1505 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 57/80, Loss: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0152\n",
      "INFO:swin_transformer_3d:Epoch 57 took 134.5212 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/80: 100%|██████████| 80/80 [01:51<00:00,  1.40s/batch, Batch Time=0.1296 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 58/80, Loss: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0174\n",
      "INFO:swin_transformer_3d:Epoch 58 took 135.1479 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/80: 100%|██████████| 80/80 [01:44<00:00,  1.31s/batch, Batch Time=0.1590 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 59/80, Loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.10s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0162\n",
      "INFO:swin_transformer_3d:Epoch 59 took 126.8869 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/80: 100%|██████████| 80/80 [01:41<00:00,  1.27s/batch, Batch Time=0.1264 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 60/80, Loss: 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0219\n",
      "INFO:swin_transformer_3d:Epoch 60 took 124.9292 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/80: 100%|██████████| 80/80 [01:44<00:00,  1.31s/batch, Batch Time=0.1307 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 61/80, Loss: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0198\n",
      "INFO:swin_transformer_3d:Epoch 61 took 127.1921 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/80: 100%|██████████| 80/80 [01:43<00:00,  1.29s/batch, Batch Time=0.1288 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 62/80, Loss: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0184\n",
      "INFO:swin_transformer_3d:Epoch 62 took 126.9685 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/80: 100%|██████████| 80/80 [01:49<00:00,  1.37s/batch, Batch Time=0.1680 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 63/80, Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.28s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0201\n",
      "INFO:swin_transformer_3d:Epoch 63 took 135.1291 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/80: 100%|██████████| 80/80 [01:52<00:00,  1.41s/batch, Batch Time=0.1518 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 64/80, Loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.27s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0204\n",
      "INFO:swin_transformer_3d:Epoch 64 took 137.9573 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/80: 100%|██████████| 80/80 [01:52<00:00,  1.41s/batch, Batch Time=0.2151 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 65/80, Loss: 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.29s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0259\n",
      "INFO:swin_transformer_3d:Epoch 65 took 138.7208 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/80: 100%|██████████| 80/80 [01:52<00:00,  1.40s/batch, Batch Time=0.1899 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 66/80, Loss: 0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0211\n",
      "INFO:swin_transformer_3d:Epoch 66 took 136.5611 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/80: 100%|██████████| 80/80 [01:49<00:00,  1.36s/batch, Batch Time=0.1660 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 67/80, Loss: 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:21<00:00,  1.06s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0177\n",
      "INFO:swin_transformer_3d:Epoch 67 took 130.4063 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/80: 100%|██████████| 80/80 [01:41<00:00,  1.26s/batch, Batch Time=0.1767 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 68/80, Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:21<00:00,  1.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0178\n",
      "INFO:swin_transformer_3d:Epoch 68 took 122.6833 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/80: 100%|██████████| 80/80 [01:47<00:00,  1.35s/batch, Batch Time=0.1771 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 69/80, Loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0181\n",
      "INFO:swin_transformer_3d:Epoch 69 took 131.9259 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/80: 100%|██████████| 80/80 [01:50<00:00,  1.38s/batch, Batch Time=0.1986 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 70/80, Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0254\n",
      "INFO:swin_transformer_3d:Epoch 70 took 135.0163 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/80: 100%|██████████| 80/80 [01:48<00:00,  1.35s/batch, Batch Time=0.1932 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 71/80, Loss: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:23<00:00,  1.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0180\n",
      "INFO:swin_transformer_3d:Epoch 71 took 132.2103 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1816 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 72/80, Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0175\n",
      "INFO:swin_transformer_3d:Epoch 72 took 129.6509 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/80: 100%|██████████| 80/80 [01:45<00:00,  1.31s/batch, Batch Time=0.1437 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 73/80, Loss: 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:22<00:00,  1.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0246\n",
      "INFO:swin_transformer_3d:Epoch 73 took 127.7729 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/80: 100%|██████████| 80/80 [01:53<00:00,  1.41s/batch, Batch Time=0.1555 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 74/80, Loss: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0159\n",
      "INFO:swin_transformer_3d:Epoch 74 took 137.5011 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/80: 100%|██████████| 80/80 [01:48<00:00,  1.36s/batch, Batch Time=0.1715 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 75/80, Loss: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0176\n",
      "INFO:swin_transformer_3d:Epoch 75 took 132.7459 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/80: 100%|██████████| 80/80 [01:50<00:00,  1.38s/batch, Batch Time=0.1568 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 76/80, Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0245\n",
      "INFO:swin_transformer_3d:Epoch 76 took 135.5688 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/80: 100%|██████████| 80/80 [02:03<00:00,  1.55s/batch, Batch Time=0.2116 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 77/80, Loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0361\n",
      "INFO:swin_transformer_3d:Epoch 77 took 148.4844 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/80: 100%|██████████| 80/80 [01:51<00:00,  1.40s/batch, Batch Time=0.1558 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 78/80, Loss: 0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:26<00:00,  1.34s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0163\n",
      "INFO:swin_transformer_3d:Epoch 78 took 138.7180 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/80: 100%|██████████| 80/80 [01:47<00:00,  1.34s/batch, Batch Time=0.1736 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 79/80, Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:24<00:00,  1.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0180\n",
      "INFO:swin_transformer_3d:Epoch 79 took 131.6964 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/80: 100%|██████████| 80/80 [01:50<00:00,  1.38s/batch, Batch Time=0.1917 sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Epoch 80/80, Loss: 0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:25<00:00,  1.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:swin_transformer_3d:Validation Loss: 0.0160\n",
      "INFO:swin_transformer_3d:Epoch 80 took 135.2637 seconds\n",
      "INFO:swin_transformer_3d:State dict saved to C:\\Users\\scmmw\\OneDrive - University of Leeds\\Masters - 23-24\\Project\\results/swin_unetr_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "max_epochs = 80\n",
    "learning_rate = 1e-2\n",
    "patience = 3\n",
    "models = []\n",
    "epoch_losses = []\n",
    "\n",
    "# Run the training function\n",
    "model, epoch_loss = train(train_loader, test_loader, max_epochs=max_epochs, learning_rate=learning_rate, patience=patience, model=swin_model)\n",
    "\n",
    "# Store the trained model and loss values\n",
    "models.append(model)\n",
    "epoch_losses.append(epoch_loss)\n",
    "\n",
    "# Save the model and state dict after training\n",
    "model_path = f\"{ResultsPath}/swin_unetr_model.pth\"\n",
    "state_dict_path = f\"{ResultsPath}/swin_unetr_state_dict.pth\"\n",
    "\n",
    "# Save the full model\n",
    "torch.save(model, model_path)\n",
    "\n",
    "torch.save({\"state_dict\": model.state_dict()}, state_dict_path)\n",
    "logger.info(f\"State dict saved to {state_dict_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(epoch_losses, early_stop_epoch=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the training loss curve and mark the early stopping point.\n",
    "\n",
    "    Args:\n",
    "        epoch_losses (list of float): List of epoch training losses.\n",
    "        early_stop_epoch (int, optional): Epoch where early stopping occurred. Defaults to None.\n",
    "        save_path (str, optional): Path to save the plot image. Defaults to None.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(epoch_losses) + 1)\n",
    "    train_losses = epoch_losses  # Since epoch_losses is already a list of floats\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "\n",
    "    if early_stop_epoch is not None:\n",
    "        plt.axvline(x=early_stop_epoch, color='r', linestyle='--', label='Early Stopping')\n",
    "\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(epoch_loss, early_stop_epoch=None, save_path=f\"{ResultsPath}/loss_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
